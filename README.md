#  Clasificaci贸n de Rese帽as de Pel铆culas con Machine Learning y BERT

Este proyecto tiene como objetivo construir un sistema de clasificaci贸n binaria para rese帽as de pel铆culas (positiva/negativa) utilizando datos de IMDB. Se emplean modelos de aprendizaje autom谩tico cl谩sico (Logistic Regression, Random Forest, XGBoost) y un modelo avanzado basado en BERT.

##  Objetivo

Predecir si una rese帽a es positiva o negativa, maximizando la m茅trica F1, con un umbral m铆nimo de 0.85 en el conjunto de prueba.

## 锔 Tecnolog铆as usadas

- Python
- pandas, numpy, matplotlib, seaborn
- scikit-learn
- spaCy
- XGBoost
- transformers (BERT)
- Jupyter Notebook

##  Modelos aplicados

- Regresi贸n Log铆stica (TF-IDF)
- Random Forest (TF-IDF)
- XGBoost (TF-IDF)
- BERT (extractor de embeddings)

##  Preprocesamiento

- Eliminaci贸n de stopwords y signos de puntuaci贸n
- Lematizaci贸n con spaCy
- Vectorizaci贸n con TF-IDF
- Extracci贸n de embeddings con BERT para una muestra peque帽a (por limitaciones de hardware)

##  Resultados

| Modelo             | Accuracy (Test) | F1 Score (Test) |
|--------------------|-----------------|-----------------|
| Regresi贸n Log铆stica | 0.88            | 0.88            |
| Random Forest       | 0.84            | 0.84            |
| XGBoost             | 0.83            | 0.83            |
| BERT (200 muestras) | 0.83            | 0.85            |

Todos los modelos cumplen con el objetivo del proyecto.

## И Clasificaci贸n de Nuevas Rese帽as

Se probaron predicciones sobre nuevas rese帽as ficticias para comparar el comportamiento de los modelos. Se observa que:

- LR es conservador.
- RF y XGB son permisivos.
- BERT ofrece balance entre precisi贸n y sensibilidad.

##  Estructura del proyecto
film-review-classifier/

 Proyecto_16.ipynb

 README.md

 requirements.txt


锔 **Nota sobre los datos:**  
El dataset no est谩 incluido en el repositorio por razones de tama帽o, pero todas las transformaciones y pasos est谩n desarrollados completamente en el notebook.

 Resultados:

Se logr贸 el objetivo del proyecto, obtener la m茅trica F1 con un umbral m铆nimo de 0.85

Los modelos basados en 谩rboles (Random Forest y XGBoost) tienden a ser m谩s permisivos al clasificar rese帽as ambiguas o neutras como positivas, mientras que la regresi贸n log铆stica es m谩s conservadora.

BERT mostr贸 un comportamiento m谩s equilibrado, aunque tambi茅n puede fallar en detectar ciertos matices de lenguaje positivo impl铆cito. En conjunto, usar BERT podr铆a aportar m谩s robustez al clasificador final, especialmente si se integra con otros modelos o se calibra correctamente el umbral de decisi贸n.

锔 Autor
Jorge Elian Mendoza Pujol

 [LinkedIn](www.linkedin.com/in/jorge-elian-mendoza-pujol-359500283) 
